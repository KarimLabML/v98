---
title: Stochastic Nonconvex Optimization with Large Minibatches
abstract: We study stochastic optimization of nonconvex loss functions, which are
  typical objectives for training neural networks. We propose stochastic approximation
  algorithms which optimize a series of regularized, nonlinearized losses on large
  minibatches of samples, using only first-order gradient information. Our algorithms
  provably converge to an approximate critical point of the expected objective with
  faster rates than minibatch stochastic gradient descent, and facilitate better parallelization
  by allowing larger minibatches.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: wang10a
month: 0
tex_title: Stochastic Nonconvex Optimization with Large Minibatches
firstpage: 856
lastpage: 881
page: 856-881
order: 856
cycles: false
bibtex_author: Wang, Weiran and Srebro, Nathan
author:
- given: Weiran
  family: Wang
- given: Nathan
  family: Srebro
date: 2010-03-09
address: 
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Algorithmic Learning
  Theory
volume: '98'
genre: inproceedings
issued:
  date-parts:
  - 2010
  - 3
  - 9
pdf: http://proceedings.mlr.press/v98/wang10a/wang10a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
