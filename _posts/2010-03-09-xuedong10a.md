---
title: General parallel optimization a without metric
abstract: Hierarchical bandits are an approach for global optimization of \emph{extremely}
  irregular functions. This paper provides new elements regarding POO, an adaptive
  meta-algorithm that does not require the knowledge of local smoothness of the target
  function. We first highlight the fact that the subroutine algorithm used in POO
  should have a small regret under the assumption of \emph{local smoothness with respect
  to the chosen partitioning}, which is unknown if it is satisfied by the standard
  subroutine HOO. In this work,  we establish such regret guarantee for HCT, which
  is another hierarchical optimistic optimization algorithm that needs to know the
  smoothness. This confirms the validity of POO.  We show that POO can be used with
  HCT as a subroutine with a regret upper bound that matches the one of best-known
  algorithms using the knowledge of smoothness up to a $\sqrt{\log{n}}$ factor. On
  top of that, we further propose a more general wrapper, called GPO, that can cope
  with algorithms that only have simple regret guarantees.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: xuedong10a
month: 0
tex_title: General parallel optimization a without metric
firstpage: 762
lastpage: 787
page: 762-787
order: 762
cycles: false
bibtex_author: Xuedong, Shang and Kaufmann, Emilie and Valko, Michal
author:
- given: Shang
  family: Xuedong
- given: Emilie
  family: Kaufmann
- given: Michal
  family: Valko
date: 2010-03-09
address: 
publisher: PMLR
container-title: Proceedings of the 30th International Conference on Algorithmic Learning
  Theory
volume: '98'
genre: inproceedings
issued:
  date-parts:
  - 2010
  - 3
  - 9
pdf: http://proceedings.mlr.press/v98/xuedong10a/xuedong10a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
